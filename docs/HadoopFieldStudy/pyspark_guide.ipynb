{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Python Version in the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Documentation -- https://spark.apache.org/docs/1.6.1/sql-programming-guide.html\n",
    "import platform\n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "###os.environ['SPARK_PREPEND_CLASSES']='/opt/sqljdbc_4.0/enu/sqljdbc4.jar'\n",
    "os.environ['SPARK_CLASSPATH']='/opt/sqljdbc_4.0/enu/sqljdbc4.jar'\n",
    "os.environ['SPARK_HOME']='/usr/hdp/2.4.2.0-258/spark'\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include ALL packages to be imported to initialize and start pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.11:1.2.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pyspark module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import additional modules within pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import py4j\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Spark in various modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation: http://spark.apache.org/docs/latest/submitting-applications.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Spark locally (local system = Head node )\n",
    "### local[32] - Run on 32 cores (on the Head node)\n",
    "### local[*] - Run on all available cores (on the Head node)\n",
    "\n",
    "### sc is the spark context is the entry point for Spark's interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1.6.1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"My App\")\n",
    "sc = SparkContext(conf = conf)\n",
    "#Check spark context version\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SQL context using Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlctx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hive Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HiveContext is a super set of SQLContext that allows you to interact with Hive using SQL and Hive QL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "\n",
    "hsqlctx = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions offered by pyspark to manipulate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All of these functions behave similar to their Pandas counterparts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation: https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data from a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading CSV using textFile function in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading files using textFile function in Spark returns RDD (Resilient Distributed Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Employee_rdd = sc.textFile(\"/user/data/state13.csv\")\n",
    "\n",
    "# use map func to split each line in csv file using comma\n",
    "\n",
    "Employee_rdd2 = Employee_rdd.map(lambda line: line.split(','))\n",
    "\n",
    "# The csv file has a header. The below code gets the header\n",
    "\n",
    "header = Employee_rdd2.first()\n",
    "\n",
    "# Removes header from file from the RDD\n",
    "\n",
    "Employee_rdd3 = Employee_rdd2.filter(lambda line: line!= header)\n",
    "\n",
    "# take returns a list of lists [['','',''...],[''],['']]  \n",
    "# take(1) returns the first row and [0] returns the first element which is the header \n",
    "\n",
    "Employee_rdd2.take(1)[0]\n",
    "\n",
    "# Converting RDD to DataFrame\n",
    "\n",
    "Employee_df = Employee_rdd3.toDF(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading CSV using utilities like Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading files using Databricks utility in Spark returns a Dataframe and it can be done using either one: SQLContext or HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using HiveContext\n",
    "df1 = hsqlctx.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('/user/data/state13.csv')\n",
    "\n",
    "# using SQLContext\n",
    "df2 = sqlctx.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('/user/data/state13.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsetting the data - select only  a few columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the df.select( ) function and pass the list of columns to be displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TimeZone=u'Eastern', Latitude=u'30.869486'),\n",
       " Row(TimeZone=u'Eastern', Latitude=u'33.030828'),\n",
       " Row(TimeZone=u'Eastern', Latitude=u'33.750591'),\n",
       " Row(TimeZone=u'Eastern', Latitude=u'34.034049'),\n",
       " Row(TimeZone=u'Eastern', Latitude=u'32.44509'),\n",
       " Row(TimeZone=u'Eastern', Latitude=u'30.869486'),\n",
       " Row(TimeZone=u'Eastern', Latitude=u'34.254602'),\n",
       " Row(TimeZone=u'Eastern', Latitude=u'33.777293'),\n",
       " Row(TimeZone=u'Eastern', Latitude=u'32.7921'),\n",
       " Row(TimeZone=u'Eastern', Latitude=u'34.177555')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Employee_df.select(['TimeZone', 'Latitude']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CityName=u'Fitzgerald'),\n",
       " Row(CityName=u'Thomson'),\n",
       " Row(CityName=u'Alto'),\n",
       " Row(CityName=u'Sargent'),\n",
       " Row(CityName=u'Alpharetta'),\n",
       " Row(CityName=u'Jeffersonville'),\n",
       " Row(CityName=u'Folkston'),\n",
       " Row(CityName=u'Moreland'),\n",
       " Row(CityName=u'Broxton'),\n",
       " Row(CityName=u'Crawfordville'),\n",
       " Row(CityName=u'Georgetown'),\n",
       " Row(CityName=u'Montezuma'),\n",
       " Row(CityName=u'Marble Hill'),\n",
       " Row(CityName=u'Lagrange'),\n",
       " Row(CityName=u'Commerce'),\n",
       " Row(CityName=u'Young Harris'),\n",
       " Row(CityName=u'Springfield'),\n",
       " Row(CityName=u'Riceboro'),\n",
       " Row(CityName=u'Oakwood'),\n",
       " Row(CityName=u'Wrens'),\n",
       " Row(CityName=u'Hardwick'),\n",
       " Row(CityName=u'Morganton'),\n",
       " Row(CityName=u'Funston'),\n",
       " Row(CityName=u'Offerman'),\n",
       " Row(CityName=u'Grantville'),\n",
       " Row(CityName=u'Molena'),\n",
       " Row(CityName=u'Kingston'),\n",
       " Row(CityName=u'Mansfield'),\n",
       " Row(CityName=u'Buena Vista'),\n",
       " Row(CityName=u'Hiawassee'),\n",
       " Row(CityName=u'Shannon'),\n",
       " Row(CityName=u'Ray City'),\n",
       " Row(CityName=u'Bowdon Junction'),\n",
       " Row(CityName=u'Patterson'),\n",
       " Row(CityName=u'Royston'),\n",
       " Row(CityName=u'Moultrie'),\n",
       " Row(CityName=u'Richmond Hill'),\n",
       " Row(CityName=u'Sparta'),\n",
       " Row(CityName=u'Rex'),\n",
       " Row(CityName=u'Waynesville'),\n",
       " Row(CityName=u'Augusta'),\n",
       " Row(CityName=u'Ellaville'),\n",
       " Row(CityName=u'Dalton'),\n",
       " Row(CityName=u'Reidsville'),\n",
       " Row(CityName=u'Camilla'),\n",
       " Row(CityName=u'Fortson'),\n",
       " Row(CityName=u'Lavonia'),\n",
       " Row(CityName=u'Armuchee'),\n",
       " Row(CityName=u'Baxley'),\n",
       " Row(CityName=u'Villa Rica'),\n",
       " Row(CityName=u'Waynesboro'),\n",
       " Row(CityName=u'Kite'),\n",
       " Row(CityName=u'Brooklet'),\n",
       " Row(CityName=u'Brooks'),\n",
       " Row(CityName=u'Bostwick'),\n",
       " Row(CityName=u'Covington'),\n",
       " Row(CityName=u'Jesup'),\n",
       " Row(CityName=u'Midland'),\n",
       " Row(CityName=u'Lake Park'),\n",
       " Row(CityName=u'Surrency'),\n",
       " Row(CityName=u'Pine Mountain'),\n",
       " Row(CityName=u'Statesboro'),\n",
       " Row(CityName=u'Kathleen'),\n",
       " Row(CityName=u'Comer'),\n",
       " Row(CityName=u'Blakely'),\n",
       " Row(CityName=u'Lyons'),\n",
       " Row(CityName=u'Sylvania'),\n",
       " Row(CityName=u'Aragon'),\n",
       " Row(CityName=u'Morven'),\n",
       " Row(CityName=u'Loganville'),\n",
       " Row(CityName=u'Homer'),\n",
       " Row(CityName=u'Sugar Valley'),\n",
       " Row(CityName=u'Silver Creek'),\n",
       " Row(CityName=u'Tallapoosa'),\n",
       " Row(CityName=u'Whitesburg'),\n",
       " Row(CityName=u'West Green'),\n",
       " Row(CityName=u'Forest Park'),\n",
       " Row(CityName=u'Manor'),\n",
       " Row(CityName=u'Cartersville'),\n",
       " Row(CityName=u'Portal'),\n",
       " Row(CityName=u'Dahlonega'),\n",
       " Row(CityName=u'Ball Ground'),\n",
       " Row(CityName=u'Lexington'),\n",
       " Row(CityName=u'Suwanee'),\n",
       " Row(CityName=u'Plainville'),\n",
       " Row(CityName=u'Tyrone'),\n",
       " Row(CityName=u'Avondale Estates'),\n",
       " Row(CityName=u'Millwood'),\n",
       " Row(CityName=u'Talbotton'),\n",
       " Row(CityName=u'Doerun'),\n",
       " Row(CityName=u'Fort Benning'),\n",
       " Row(CityName=u'Glennville'),\n",
       " Row(CityName=u'Eastman'),\n",
       " Row(CityName=u'Eastanollee'),\n",
       " Row(CityName=u'Howard'),\n",
       " Row(CityName=u'Flowery Branch'),\n",
       " Row(CityName=u'Thomasville'),\n",
       " Row(CityName=u'Franklin Springs'),\n",
       " Row(CityName=u'Ellenton'),\n",
       " Row(CityName=u'Appling'),\n",
       " Row(CityName=u'Pitts'),\n",
       " Row(CityName=u'Atlanta'),\n",
       " Row(CityName=u'Willacoochee'),\n",
       " Row(CityName=u'Adel'),\n",
       " Row(CityName=u'Canton'),\n",
       " Row(CityName=u'Mc Rae Helena'),\n",
       " Row(CityName=u'Glenwood'),\n",
       " Row(CityName=u'Warm Springs'),\n",
       " Row(CityName=u'Cleveland'),\n",
       " Row(CityName=u'Meldrim'),\n",
       " Row(CityName=u'Hortense'),\n",
       " Row(CityName=u'Palmetto'),\n",
       " Row(CityName=u'Grayson'),\n",
       " Row(CityName=u'Smyrna'),\n",
       " Row(CityName=u'Louisville'),\n",
       " Row(CityName=u'Holly Springs'),\n",
       " Row(CityName=u'Tate'),\n",
       " Row(CityName=u'Woodbury'),\n",
       " Row(CityName=u'Omega'),\n",
       " Row(CityName=u'Dublin'),\n",
       " Row(CityName=u'Braselton'),\n",
       " Row(CityName=u'Tifton'),\n",
       " Row(CityName=u'Rochelle'),\n",
       " Row(CityName=u'Fort Valley'),\n",
       " Row(CityName=u'Macon'),\n",
       " Row(CityName=u'Eatonton'),\n",
       " Row(CityName=u'Pinehurst'),\n",
       " Row(CityName=u'Unadilla'),\n",
       " Row(CityName=u'Athens'),\n",
       " Row(CityName=u'Lawrenceville'),\n",
       " Row(CityName=u'Hampton'),\n",
       " Row(CityName=u'Toccoa'),\n",
       " Row(CityName=u'Cordele'),\n",
       " Row(CityName=u'Norman Park'),\n",
       " Row(CityName=u'Nashville'),\n",
       " Row(CityName=u'Plains'),\n",
       " Row(CityName=u'Sycamore'),\n",
       " Row(CityName=u'Hagan'),\n",
       " Row(CityName=u'Lakeland'),\n",
       " Row(CityName=u'Pine Mountain Valley'),\n",
       " Row(CityName=u'Peachtree Corners'),\n",
       " Row(CityName=u'Adairsville'),\n",
       " Row(CityName=u'Waleska'),\n",
       " Row(CityName=u'Reynolds'),\n",
       " Row(CityName=u'Blairsville'),\n",
       " Row(CityName=u'Buford'),\n",
       " Row(CityName=u'Norcross'),\n",
       " Row(CityName=u'Saint Simons Island'),\n",
       " Row(CityName=u'Butler'),\n",
       " Row(CityName=u'Vidalia'),\n",
       " Row(CityName=u'Shellman'),\n",
       " Row(CityName=u'Lumpkin'),\n",
       " Row(CityName=u'Sardis'),\n",
       " Row(CityName=u'Marshallville'),\n",
       " Row(CityName=u'Fort Stewart'),\n",
       " Row(CityName=u'Zebulon'),\n",
       " Row(CityName=u'Milner'),\n",
       " Row(CityName=u'Kennesaw'),\n",
       " Row(CityName=u'Ty Ty'),\n",
       " Row(CityName=u'Douglas'),\n",
       " Row(CityName=u'Hinesville'),\n",
       " Row(CityName=u'Newborn'),\n",
       " Row(CityName=u'West Point'),\n",
       " Row(CityName=u'Midway'),\n",
       " Row(CityName=u'Rabun Gap'),\n",
       " Row(CityName=u'Riverdale'),\n",
       " Row(CityName=u'Clayton'),\n",
       " Row(CityName=u'Carrollton'),\n",
       " Row(CityName=u'Maysville'),\n",
       " Row(CityName=u'Ashburn'),\n",
       " Row(CityName=u'Waverly Hall'),\n",
       " Row(CityName=u'Calhoun'),\n",
       " Row(CityName=u'Ellijay'),\n",
       " Row(CityName=u'Manchester'),\n",
       " Row(CityName=u'Lenox'),\n",
       " Row(CityName=u'Bowersville'),\n",
       " Row(CityName=u'Metter'),\n",
       " Row(CityName=u'Darien'),\n",
       " Row(CityName=u'Dixie'),\n",
       " Row(CityName=u'Union Point'),\n",
       " Row(CityName=u'Mount Airy'),\n",
       " Row(CityName=u'Lumber City'),\n",
       " Row(CityName=u'Ailey'),\n",
       " Row(CityName=u'Snellville'),\n",
       " Row(CityName=u'Stapleton'),\n",
       " Row(CityName=u'Pembroke'),\n",
       " Row(CityName=u'Claxton'),\n",
       " Row(CityName=u'Preston'),\n",
       " Row(CityName=u'Twin City'),\n",
       " Row(CityName=u'McDonough'),\n",
       " Row(CityName=u'Dexter'),\n",
       " Row(CityName=u'Pearson'),\n",
       " Row(CityName=u'Buchanan'),\n",
       " Row(CityName=u'Winston'),\n",
       " Row(CityName=u'Centerville'),\n",
       " Row(CityName=u'Hephzibah'),\n",
       " Row(CityName=u'Elko'),\n",
       " Row(CityName=u'Hazlehurst'),\n",
       " Row(CityName=u'Donalsonville'),\n",
       " Row(CityName=u'Union City'),\n",
       " Row(CityName=u'Morgan'),\n",
       " Row(CityName=u'Moody AFB'),\n",
       " Row(CityName=u'Colquitt'),\n",
       " Row(CityName=u'Oglethorpe'),\n",
       " Row(CityName=u'Bishop'),\n",
       " Row(CityName=u'Danielsville'),\n",
       " Row(CityName=u'Hoboken'),\n",
       " Row(CityName=u'Lula'),\n",
       " Row(CityName=u'Powder Springs'),\n",
       " Row(CityName=u'Cedar Springs'),\n",
       " Row(CityName=u'Richland'),\n",
       " Row(CityName=u'Irwinville'),\n",
       " Row(CityName=u'Jefferson'),\n",
       " Row(CityName=u'Sandersville'),\n",
       " Row(CityName=u'Jersey'),\n",
       " Row(CityName=u'Sylvester'),\n",
       " Row(CityName=u'Jackson'),\n",
       " Row(CityName=u'Bogart'),\n",
       " Row(CityName=u'Greensboro'),\n",
       " Row(CityName=u'Roopville'),\n",
       " Row(CityName=u'Elberton'),\n",
       " Row(CityName=u'La Fayette'),\n",
       " Row(CityName=u'Kings Bay'),\n",
       " Row(CityName=u'Porterdale'),\n",
       " Row(CityName=u'Haddock'),\n",
       " Row(CityName=u'Stockbridge'),\n",
       " Row(CityName=u'Homerville'),\n",
       " Row(CityName=u'Dallas'),\n",
       " Row(CityName=u'Menlo'),\n",
       " Row(CityName=u'Cornelia'),\n",
       " Row(CityName=u'Stone Mountain'),\n",
       " Row(CityName=u'Tybee Island'),\n",
       " Row(CityName=u'Hawkinsville'),\n",
       " Row(CityName=u'Pooler'),\n",
       " Row(CityName=u'Ellenwood'),\n",
       " Row(CityName=u'Morrow'),\n",
       " Row(CityName=u'Perry'),\n",
       " Row(CityName=u'Jekyll Island'),\n",
       " Row(CityName=u'Taylorsville'),\n",
       " Row(CityName=u'Chatsworth'),\n",
       " Row(CityName=u'Warner Robins'),\n",
       " Row(CityName=u'Ludowici'),\n",
       " Row(CityName=u'Hamilton'),\n",
       " Row(CityName=u'Chester'),\n",
       " Row(CityName=u'Albany'),\n",
       " Row(CityName=u'Rome'),\n",
       " Row(CityName=u'Watkinsville'),\n",
       " Row(CityName=u'Waco'),\n",
       " Row(CityName=u'Alapaha'),\n",
       " Row(CityName=u'Woodstock'),\n",
       " Row(CityName=u'Dry Branch'),\n",
       " Row(CityName=u'Resaca'),\n",
       " Row(CityName=u'Brookfield'),\n",
       " Row(CityName=u'Griffin'),\n",
       " Row(CityName=u'Waycross'),\n",
       " Row(CityName=u'Jasper'),\n",
       " Row(CityName=u'Newnan'),\n",
       " Row(CityName=u'Newton'),\n",
       " Row(CityName=u'Valdosta'),\n",
       " Row(CityName=u'Mount Berry'),\n",
       " Row(CityName=u'Thomaston'),\n",
       " Row(CityName=u'Bellville'),\n",
       " Row(CityName=u'Pine Lake'),\n",
       " Row(CityName=u'Sea Island'),\n",
       " Row(CityName=u'Leesburg'),\n",
       " Row(CityName=u'Greenville'),\n",
       " Row(CityName=u'White'),\n",
       " Row(CityName=u'Lithia Springs'),\n",
       " Row(CityName=u'Trion'),\n",
       " Row(CityName=u'Red Oak'),\n",
       " Row(CityName=u'Danville'),\n",
       " Row(CityName=u'Arnoldsville'),\n",
       " Row(CityName=u'Gainesville'),\n",
       " Row(CityName=u'Monroe'),\n",
       " Row(CityName=u'Fayetteville'),\n",
       " Row(CityName=u'Dudley'),\n",
       " Row(CityName=u'Arabi'),\n",
       " Row(CityName=u'Gay'),\n",
       " Row(CityName=u'Alamo'),\n",
       " Row(CityName=u'Newington'),\n",
       " Row(CityName=u'Irwinton'),\n",
       " Row(CityName=u'Byron'),\n",
       " Row(CityName=u'Crawford'),\n",
       " Row(CityName=u'Geneva'),\n",
       " Row(CityName=u'East Dublin'),\n",
       " Row(CityName=u'Rebecca'),\n",
       " Row(CityName=u'Auburn'),\n",
       " Row(CityName=u'Scottdale'),\n",
       " Row(CityName=u'Monticello'),\n",
       " Row(CityName=u'The Rock'),\n",
       " Row(CityName=u'Fort Gaines'),\n",
       " Row(CityName=u'Quitman'),\n",
       " Row(CityName=u'Locust Grove'),\n",
       " Row(CityName=u'Gordon'),\n",
       " Row(CityName=u'Bowdon'),\n",
       " Row(CityName=u'Byromville'),\n",
       " Row(CityName=u'Acworth'),\n",
       " Row(CityName=u'Gray'),\n",
       " Row(CityName=u'Clarkesville'),\n",
       " Row(CityName=u'Martin'),\n",
       " Row(CityName=u'Blue Ridge'),\n",
       " Row(CityName=u'Hull'),\n",
       " Row(CityName=u'Mc Intyre'),\n",
       " Row(CityName=u'Bethlehem'),\n",
       " Row(CityName=u'Barnesville'),\n",
       " Row(CityName=u'Williamson'),\n",
       " Row(CityName=u'Dawsonville'),\n",
       " Row(CityName=u'Forsyth'),\n",
       " Row(CityName=u'Summerville'),\n",
       " Row(CityName=u'Roswell'),\n",
       " Row(CityName=u'Arlington'),\n",
       " Row(CityName=u'Rincon'),\n",
       " Row(CityName=u'Senoia'),\n",
       " Row(CityName=u'Stillmore'),\n",
       " Row(CityName=u'East Ellijay'),\n",
       " Row(CityName=u'Nicholls'),\n",
       " Row(CityName=u'Wrightsville'),\n",
       " Row(CityName=u'Bloomingdale'),\n",
       " Row(CityName=u'Pelham'),\n",
       " Row(CityName=u'Millen'),\n",
       " Row(CityName=u'Duluth'),\n",
       " Row(CityName=u'Marietta'),\n",
       " Row(CityName=u'Jonesboro'),\n",
       " Row(CityName=u'Franklin'),\n",
       " Row(CityName=u'Columbus'),\n",
       " Row(CityName=u'Decatur'),\n",
       " Row(CityName=u'Lithonia'),\n",
       " Row(CityName=u'Bowman'),\n",
       " Row(CityName=u'Sunny Side'),\n",
       " Row(CityName=u'Bonaire'),\n",
       " Row(CityName=u'Sautee Nacoochee'),\n",
       " Row(CityName=u'Evans'),\n",
       " Row(CityName=u'Colbert'),\n",
       " Row(CityName=u'Winterville'),\n",
       " Row(CityName=u'Fairmount'),\n",
       " Row(CityName=u'Baldwin'),\n",
       " Row(CityName=u'Dacula'),\n",
       " Row(CityName=u'Lilburn'),\n",
       " Row(CityName=u'Ocilla'),\n",
       " Row(CityName=u'Mableton'),\n",
       " Row(CityName=u'Turin'),\n",
       " Row(CityName=u'Winder'),\n",
       " Row(CityName=u'Ellerslie'),\n",
       " Row(CityName=u'Pineview'),\n",
       " Row(CityName=u'Mount Zion'),\n",
       " Row(CityName=u'Conley'),\n",
       " Row(CityName=u'Cohutta'),\n",
       " Row(CityName=u'Sparks'),\n",
       " Row(CityName=u'Helen'),\n",
       " Row(CityName=u'Vienna'),\n",
       " Row(CityName=u'Ellabell'),\n",
       " Row(CityName=u'Dawson'),\n",
       " Row(CityName=u'Rutledge'),\n",
       " Row(CityName=u'Culloden'),\n",
       " Row(CityName=u'Sharpsburg'),\n",
       " Row(CityName=u'Luthersville'),\n",
       " Row(CityName=u'Blackshear'),\n",
       " Row(CityName=u'Pavo'),\n",
       " Row(CityName=u'Port Wentworth'),\n",
       " Row(CityName=u'Hiram'),\n",
       " Row(CityName=u'Fairburn'),\n",
       " Row(CityName=u'Ambrose'),\n",
       " Row(CityName=u'Ila'),\n",
       " Row(CityName=u'Lindale'),\n",
       " Row(CityName=u'Temple'),\n",
       " Row(CityName=u'Tennille'),\n",
       " Row(CityName=u'Leslie'),\n",
       " Row(CityName=u'Milledgeville'),\n",
       " Row(CityName=u'Rentz'),\n",
       " Row(CityName=u'Coosa'),\n",
       " Row(CityName=u'Hahira'),\n",
       " Row(CityName=u'Hoschton'),\n",
       " Row(CityName=u'Soperton'),\n",
       " Row(CityName=u'Cuthbert'),\n",
       " Row(CityName=u'Cochran'),\n",
       " Row(CityName=u'Tiger'),\n",
       " Row(CityName=u'Rydal'),\n",
       " Row(CityName=u'Statham'),\n",
       " Row(CityName=u'Wadley'),\n",
       " Row(CityName=u'Alma'),\n",
       " Row(CityName=u'Demorest'),\n",
       " Row(CityName=u'Montrose'),\n",
       " Row(CityName=u'Lizella'),\n",
       " Row(CityName=u'Savannah'),\n",
       " Row(CityName=u'Guyton'),\n",
       " Row(CityName=u'Juliette'),\n",
       " Row(CityName=u'Nahunta'),\n",
       " Row(CityName=u'Hogansville'),\n",
       " Row(CityName=u'Screven'),\n",
       " Row(CityName=u'Rockmart'),\n",
       " Row(CityName=u'Statenville'),\n",
       " Row(CityName=u'Emerson'),\n",
       " Row(CityName=u'Chula'),\n",
       " Row(CityName=u'Pendergrass'),\n",
       " Row(CityName=u'Dillard'),\n",
       " Row(CityName=u'Roberta'),\n",
       " Row(CityName=u'Fargo'),\n",
       " Row(CityName=u'Tucker'),\n",
       " Row(CityName=u'Madison'),\n",
       " Row(CityName=u'Clarkston'),\n",
       " Row(CityName=u'Saint Marys'),\n",
       " Row(CityName=u'Damascus'),\n",
       " Row(CityName=u'Carnesville'),\n",
       " Row(CityName=u'Warwick'),\n",
       " Row(CityName=u'Bluffton'),\n",
       " Row(CityName=u'Baconton'),\n",
       " Row(CityName=u'Hartwell'),\n",
       " Row(CityName=u'Brunswick'),\n",
       " Row(CityName=u'Townsend'),\n",
       " Row(CityName=u'Cedartown'),\n",
       " Row(CityName=u'Davisboro'),\n",
       " Row(CityName=u'Conyers'),\n",
       " Row(CityName=u'Kingsland'),\n",
       " Row(CityName=u'Cataula'),\n",
       " Row(CityName=u'Bainbridge'),\n",
       " Row(CityName=u'Cumming'),\n",
       " Row(CityName=u'Austell'),\n",
       " Row(CityName=u'Cave Spring'),\n",
       " Row(CityName=u'Lyerly'),\n",
       " Row(CityName=u'Lovejoy'),\n",
       " Row(CityName=u'Americus'),\n",
       " Row(CityName=u'Mount Vernon'),\n",
       " Row(CityName=u'Douglasville'),\n",
       " Row(CityName=u'Andersonville'),\n",
       " Row(CityName=u'Swainsboro'),\n",
       " Row(CityName=u'Oxford'),\n",
       " Row(CityName=u'Abbeville'),\n",
       " Row(CityName=u'Jenkinsburg'),\n",
       " Row(CityName=u'Cusseta'),\n",
       " Row(CityName=u'Warrenton'),\n",
       " Row(CityName=u'Dover'),\n",
       " Row(CityName=u'Turnerville'),\n",
       " Row(CityName=u'Box Springs'),\n",
       " Row(CityName=u'Edison'),\n",
       " Row(CityName=u'Murrayville'),\n",
       " Row(CityName=u'Bremen'),\n",
       " Row(CityName=u'Social Circle'),\n",
       " Row(CityName=u'Milan'),\n",
       " Row(CityName=u'Nicholson'),\n",
       " Row(CityName=u'Grovetown'),\n",
       " Row(CityName=u'Tallulah Falls'),\n",
       " Row(CityName=u'Enigma'),\n",
       " Row(CityName=u'Peachtree City'),\n",
       " Row(CityName=u'Toccoa Falls')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Employee_df.select('CityName').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### df.show(n), df.take(n), df.head(n) - displays first n records "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df.collect( ) - Displays whole dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe API has no function tail( ), last( ) to get the last few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Evaluation in Spark\n",
    "\n",
    "All Spark operations can be classified into two categories: (1) Transformations (2) Actions\n",
    "\n",
    "Spark creates a DAG (Direct Acyclic Graph) for all \"Transformation\" operations and it executes (all the operations from the beginning) once an \"Action\" operation is invoked, hence the term \"Lazy Evaluation\".\n",
    "\n",
    "### What are some examples of transformation operations ?\n",
    "\n",
    "Reading data from csv or Hive to create Resilient Distributed Dataset (RDD) or DataFrame (DF).\n",
    "   \n",
    "Creating new columns in RDD or DF.\n",
    "   \n",
    "Any operation that does not return results to the user.\n",
    "    \n",
    "### What are some examples of action operations? \n",
    "\n",
    "Calling head( ), take( ), or collect( ) on a dataframe or RDD.\n",
    "   \n",
    "### NOTE: Don't be surprised if none of the commands throw errors or exceptions until head( ) has been called on the result.  Only then does the real execution takes place and errors revealed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from Hive tables using Hive Queries\n",
    "### Use hsqlctx.sql(query) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = hsqlctx.sql('select * from redcrossteam2.all_states_2000_2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation of data\n",
    "\n",
    "Similar to Pandas aggregation. Use groupBy or groupby functions in dataframe. Note that groupBy (with a capital B) and groupby are aliases of each other. \n",
    "\n",
    "Refer to the documentation for more details: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pro Tip for Aggregation\n",
    "\n",
    "Always use .agg( ) function along with groupby or groupBy. Why?  It gives more flexibility. \n",
    "It gives the capability to mimic the SQL groupby functionality including renaming the aggregate columns and multiple aggregations on one column.\n",
    "\n",
    "For example,\n",
    "\n",
    "consider the SQL statement\n",
    "\n",
    "select count(*) as num_states, min(donation_dt) as min_don_dt,max(donation_dt) as max_don_dt from table_name group by            StateName;\n",
    "\n",
    "What does this SQL do? \n",
    "It retrieves the number of records, maximum donation date, and minimum donation date for each state\n",
    "Notice that donation_dt is used in 2 different aggregate functions and selected as 2 different aggregate colummns\n",
    "\n",
    "This can be translated into a df group by function as shown below\n",
    "    \n",
    "df.groupBy('StateName').agg({'StateName' : {'num_states' : 'count'},'donation_dt':  {'min_donation_dt' : 'min' , 'max_donation_dt' : 'max'}}).collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(arc_id=65264167, donation_dt=datetime.date(2005, 2, 10), prodctv_proc_ind=1, first_donat_ind=0, deferral_ind=0, donation_ind=1, sponsor_name=u'University of Georgia Hill Community', sponsor_category=u'EDUCATION', site_zip=30609, donation_type=u'Red Cell Apheresis', ziptype=u'U', cityname=u'Athens', statename=u'Georgia'),\n",
       " Row(arc_id=84098182, donation_dt=datetime.date(2008, 3, 20), prodctv_proc_ind=1, first_donat_ind=1, deferral_ind=0, donation_ind=1, sponsor_name=u'Shiloh High School', sponsor_category=u'EDUCATION', site_zip=30039, donation_type=u'Whole Blood', ziptype=u'S', cityname=u'Snellville', statename=u'Georgia')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df.groupBy('statename').agg({'donation_dt' :'min','statename' : 'count','donation_dt' :'max'}).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##df.groupBy('statename').agg({'statename' : {'num_states' : 'count'},'donation_dt':  {'min_donation_dt' : 'min' , 'max_donation_dt' : 'max'}}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Joins\n",
    "\n",
    "use df.join( ) function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pro Tip for Join\n",
    "\n",
    "Always create aliases for dataframes being joined using df.alias( ) function before the join operation. eg., left_df = df1.alias('left_df').\n",
    "\n",
    "Use aliases in the join operation. \n",
    "join() function has to be called using a dataframe object and takes 3 arguments, the second dataframe to be joined, join condition and the type of join to be performed.\n",
    "\n",
    "For multiple joins, use a list of conditions. for eg., cond = [left_df.col1 == right_df.colA, left_df.col2 == right_df.colB,..]\n",
    "\n",
    "Example of a join syntax is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating aliases\n",
    "left_df = df1.alias('left_df')\n",
    "right_df =df2.alias('right_df')\n",
    "\n",
    "# join conditions\n",
    "cond = [left_df.arc_id == right_df.arc_id]\n",
    "\n",
    "# join function\n",
    "df_joined = left_df.join(right_df,cond,'inner').select([F.col(\"left_df.arc_id\"),F.col(\"right_df.bzd_assessedhomevalue\")]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Selecting all columns from one dataframe and a few columns from another dataframe in a join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating aliases\n",
    "left_df = df1.alias('left_df')\n",
    "right_df = df2.alias('right_df')\n",
    "\n",
    "# join conditions\n",
    "cond = [left_df.arc_id == right_df.arc_id]\n",
    "\n",
    "# list of columns to be selected from left and right dataframe being joined\n",
    "left_cols_to_select = [F.col('left_df.'+a) for a in left_df.columns]\n",
    "right_cols_to_select = [F.col('right_df.bzd_assessedhomevalue'),F.col('right_df.bzd_avg_inq_all12')]\n",
    "\n",
    "# join function\n",
    "df_joined = left_df.join(right_df,cond,'inner').select(left_cols_to_select+right_cols_to_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Query dataframes using SQL-like syntax\n",
    "\n",
    "\n",
    "Spark Dataframes/RDDs can be accessed using a SQL-like Syntax. There are 2 steps:\n",
    "\n",
    "1) Register the Dataframe as temp table using SQLContext(df,'tableName')\n",
    "\n",
    "2) Use SQLContext.sql( ) function to query it like a normal table\n",
    "\n",
    "\n",
    "In case of RDDs, \n",
    "\n",
    "1) Register the RDD as a temptable using the function RDDname.registerTempTable()\n",
    "\n",
    "2) Use SQLContext.sql( ) function to query it like a normal table\n",
    "\n",
    "\n",
    "Example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(zip4=7138, MinD=u'1973/08/11', MaxD=u'1973/08/11', num=1),\n",
       " Row(zip4=3546, MinD=u'1984/11/24', MaxD=u'1984/11/24', num=1),\n",
       " Row(zip4=2357, MinD=u'1976/02/01', MaxD=u'1976/02/01', num=1),\n",
       " Row(zip4=2165, MinD=u'1989/02/27', MaxD=u'1989/02/27', num=1),\n",
       " Row(zip4=1801, MinD=u'1989/03/03', MaxD=u'1989/03/03', num=1),\n",
       " Row(zip4=3406, MinD=u'1972/01/19', MaxD=u'1972/01/19', num=1),\n",
       " Row(zip4=2420, MinD=u'1982/05/07', MaxD=u'1982/05/07', num=1),\n",
       " Row(zip4=1224, MinD=u'1985/08/30', MaxD=u'1985/08/30', num=1),\n",
       " Row(zip4=2224, MinD=u'1984/03/19', MaxD=u'1984/03/19', num=1)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register as a temp table\n",
    "sqlctx.registerDataFrameAsTable(df_joined,\"dfJoinedTable\")\n",
    "\n",
    "# Use SQL query\n",
    "sqlctx.sql(\"select zip4, min(birth_dt) as MinD, max(birth_dt) as MaxD,count(zip4) as num from dfJoinedTable group by zip4\").take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "heading_collapsed": true
   },
   "source": [
    "# Creating a new column in a Spark Dataframe\n",
    "\n",
    "Use df.withColumn( ) function. \n",
    "withColumn( ) takes 2 arguments - the new column name and value to be filled in that column.\n",
    "\n",
    "A new column can be created based on other columns in the dataframe based on a business logic. In this case, do the following:\n",
    "\n",
    "1) write the business logic in a function\n",
    "\n",
    "2) convert to User Defined Function (UDF)\n",
    "\n",
    "3) pass the UDF as an argument to withColumn( ) function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the business logic as a function (date conversion in this case)\n",
    "\n",
    "from datetime import datetime\n",
    "def convDateFormat(x):\n",
    "    return datetime.strptime(x, '%Y/%m/%d')\n",
    "\n",
    "# Create a UDF\n",
    "from pyspark.sql.types import DateType\n",
    "func =  F.udf (convDateFormat, DateType())\n",
    "\n",
    "# use withColumn() to create a new column\n",
    "df1 = df1.withColumn('birth_dt_test', func(F.col('birth_dt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: \n",
    "Here is the \"functional programming\" way of converting date\n",
    "\n",
    "func =  udf ( lambda x: datetime.strptime ( x, '%Y/%m/%d'), DateType ( ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load a csv file into Hive using pyspark\n",
    "\n",
    "csv files can be loaded into Hive using pyspark HiveContext without even creating the table structure before hand. The steps are:\n",
    "\n",
    "1) Read csv into a Dataframe or RDD\n",
    "    \n",
    "2) Register the Dataframe or RDD as temp table using appropriate function (registerTempTable( ) for RDD and registerDataFrameAsTable( ) for df)\n",
    "    \n",
    "3) Load into hive by running the SQL syntax \"create table TableName as select * from TempTableName\" in HiveContext\n",
    "\n",
    "Example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv file into dataframe\n",
    "df = hsqlctx.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('/user/data/donor_summary_sample.csv')\n",
    "\n",
    "# register as temp table  (USING HIVECONTEXT! DONT USE SQLCONTEXT TO REGISTER AS TEMP TABLE)\n",
    "hsqlctx.registerDataFrameAsTable(df,\"donor_summary_tmp\")\n",
    "\n",
    "# Load into Hive creating a new table on-the-fly\n",
    "hsqlctx.sql(\"drop table donor_summary_sample\")\n",
    "hsqlctx.sql(\"create table donor_summary_sample as select * from donor_summary_tmp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bring data into memory\n",
    "\n",
    "Most of the times, the data has to be brought to memory for convenience (for eg., to visualize or to plot)\n",
    "\n",
    "Although pyspark supports plotting, it is generally preferred to have the data to be plotted in memory and use existing plotting libraries available.\n",
    "\n",
    "The data stored in spark dataframes can be brought into memory as a Pandas dataframe by calling toPandas( ) on the Spark dataframe as shown below.\n",
    "\n",
    "### Caution: Make sure that the data fits into memory before it can be brought into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_df = df1.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Spark context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun exercise\n",
    "\n",
    "Mimic the SQL statement in pyspark:\n",
    "\n",
    "select round( datediff ( donation_dt, birth_dt) / 365) as age, count(*) from all_states1_2000_2016 where round( datediff( donation_dt, birth_dt) / 365) >= 0 group by round ( datediff(donation_dt, birth_dt)/  365) order by age;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read schema from hive table\n",
    "df_all_states_h = hsqlctx.sql(\"select * from all_states1 limit 1\")\n",
    "\n",
    "#reading from Parquet file\n",
    "df_all_states = sqlctx.read.parquet('/user/data/all_states')\n",
    "\n",
    "##Rename columns of dataframe read from Parquet\n",
    "for i in range(len(df_all_states_h.columns)):\n",
    "    df_all_states = df_all_states.withColumnRenamed(df_all_states.columns[i],df_all_states_h.columns[i])\n",
    "    \n",
    "## Create age column from birth_dt and donation_dt\n",
    "\n",
    "df_all_states = df_all_states.withColumn('age',F.round(F.datediff(df_all_states.donation_dt,df_all_states.birth_dt)/365))\n",
    "\n",
    "##apply filter\n",
    "df_all_states_2000_2016 = df_all_states.where((F.year(df_all_states.donation_dt) >=2000) & (F.year(df_all_states.donation_dt) <=2016) &(df_all_states.age>=0))\n",
    "\n",
    "#group by age\n",
    "df_all_states_2000_2016.groupby('age').agg({'arc_id':'count'}).sort('age').collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
